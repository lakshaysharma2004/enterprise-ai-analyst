# Enterprise AI Knowledge Analyst

### Retrieval-Augmented Generation (RAG) System from Scratch

---

## ğŸ“Œ Project Overview

**Enterprise AI Knowledge Analyst** is an end-to-end **Retrieval-Augmented Generation (RAG)** system built from scratch to answer user queries using external documents instead of relying solely on a language modelâ€™s internal knowledge.

The project demonstrates how large language models (LLMs) can be **grounded in enterprise knowledge bases** using semantic retrieval, reducing hallucinations and improving answer reliability.

This system is designed with **modular architecture**, **clear separation of concerns**, and **industry-aligned engineering practices**.

---

## ğŸ¯ Problem Statement

Large Language Models (LLMs) often:

* hallucinate answers,
* lack access to private or up-to-date information,
* cannot explain where their answers come from.

This project solves that by:

* retrieving relevant knowledge from documents,
* injecting that knowledge into the LLM prompt,
* forcing the model to answer **only from retrieved context**.

---

## ğŸ§  Key Concept: Retrieval-Augmented Generation (RAG)

RAG combines two components:

1. **Retrieval** â€“ Find relevant document chunks using semantic similarity
2. **Generation** â€“ Generate an answer using only retrieved content

This approach:

* reduces hallucinations,
* improves trustworthiness,
* enables domain-specific knowledge access.

---

## ğŸ—ï¸ System Architecture

```
Document
   â†“
Parser
   â†“
Chunker
   â†“
Embedder
   â†“
Vector Store
   â†“
Retriever
   â†“
Generator (LLM)
   â†“
Final Answer
```

Each stage is implemented as an **independent module**, making the system extensible and easy to reason about.

---

## ğŸ“ Project Structure

```
enterprise-ai-analyst/
â”‚
â”œâ”€â”€ parsing/        # Document parsers (BaseParser, TextParser)
â”œâ”€â”€ chunking/       # Chunking logic (BaseChunker, TextChunker)
â”œâ”€â”€ core/           # Core data structures (Chunk)
â”œâ”€â”€ embeddings/     # Embedding models
â”œâ”€â”€ vectorstore/    # Vector storage & similarity search
â”œâ”€â”€ retriever/      # Retrieval logic
â”œâ”€â”€ generator/      # RAG-based answer generation
â”œâ”€â”€ config/         # Reserved for future configuration
â”‚
â”œâ”€â”€ main.py         # Entry point
â”œâ”€â”€ README.md       # Project documentation
â””â”€â”€ .gitignore
```

> Note: `tests/`, `.env`, and `venv/` are intentionally excluded from version control.

---

## ğŸ§© Implementation Stages

### Stage 1: Parsing

* Converts raw documents into structured text.
* Current support: plain text files.

### Stage 2: Chunking

* Splits documents into fixed-size chunks.
* Preserves metadata like source file and page number.

### Stage 3: Embedding

* Converts text chunks into semantic vectors.
* Uses sentence-level transformer embeddings.

### Stage 4: Vector Storage & Retrieval

* Stores embeddings in an in-memory vector store.
* Uses cosine similarity for semantic search.

### Stage 5: Generation (RAG)

* Builds a **constrained prompt** using retrieved chunks.
* Instructs the LLM to:
  * use only provided context,
  * explicitly say *â€œI donâ€™t knowâ€* if the answer is missing.

---

## ğŸ” API Key & Security

* OpenAI API key is loaded securely via environment variables.
* `.env` file is excluded from version control.
* No secrets are hard-coded.

---

## ğŸ§ª Testing Strategy

* Individual components were tested locally (parser, chunker, embedder, retriever).
* End-to-end testing validates the full RAG pipeline.
* Tests are intentionally excluded from the public repository.

---

## âš ï¸ Known Limitations

* In-memory vector store (not suitable for large-scale production).
* No streaming or conversation memory.
* API usage limited by OpenAI quota.
* Currently supports only text documents.

These limitations are **intentional** to keep the focus on learning core RAG concepts.

---

## ğŸš€ Future Improvements

* Replace in-memory vector store with FAISS or a database-backed solution.
* Add support for PDFs and DOCX files.
* Implement answer citations.
* Add logging and configuration management.
* Introduce evaluation metrics for retrieval quality.

---

## ğŸ“š Key Learnings

* Why chunk size affects retrieval quality.
* Why prompt design is critical to reduce hallucinations.
* How modular design improves maintainability.
* How retrieval and generation must be decoupled.
* How real-world constraints (API quotas) affect systems.

---

## ğŸ‘¤ Author

**Lakshay Sharma**
Final-Year B.Tech (CSE â€“ AIML)

